# RGT
Refined Generation-Based Framework for Consistent and Reliable Visual Question Answering

Distribution shift, defined as the discrepancy between the training and test answer distributions, poses a significant challenge in multi-modal tasks. Despite advances in visual question answering (VQA) and powerful pretrained large models (PLMs), distribution shift undermines model generalization and prediction accuracy. Current VQA methods perform well on in-distribution (ID) datasets but often falter on out-of-distribution (OOD) data, making continuous shift unreliable. While PLMs excel in diverse multi-modal tasks, their predictions remain insufficiently robust, and existing reliability-focused VQA methods emphasize data partitioning over data augmentation. To address these issues, we propose Refined Generation-based Training (RGT), a unified framework that jointly improves consistency and reliability. RGT integrates two components: a self-consistency mechanism within the VQA model to enhance prediction consistency, and a selection function trained with synthetic data to boost reliability. Specifically, a novel data augmentation achieved by the proposed generation pipeline compensates for the weakness among the subsets, consequently advancing the training of the selection function; a self-consistency scheme introduced into the training of the model compels the model to answer more questions according to one image, accordingly strengthening the image understanding of the model. This model-agnostic approach can be applied to augment any VQA architecture. Extensive experiments demonstrate that RGT achieves state-of-the-art performance and enhances robustness under continuous distribution shift.
[Figure0.pdf](https://github.com/user-attachments/files/21290370/Figure0.pdf)
